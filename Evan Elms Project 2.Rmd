---
title: "Project 2"
author: "Evan Elms"
date: "6/24/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(XML)
library(ZillowR)
library(readr)
library(caret)
```

# Get Addresses

## About the data

Before calling Zillow's Realestate APIs, we must first gather all possible addresses in the Las Angeles county area. Since the Bureau of Engineering maintains a [list of all registered LA addresses](https://catalog.data.gov/dataset/addresses-in-the-city-of-los-angeles/resource/e922beea-6b7a-46ab-a3fb-536dd3f6fdd5), we can download the comma-seperated values (CSV) file and extract the 1,002,025 addresses. In this file we find the following columns:  

* HSE_ID
* PIN
* PIND
* HSE_NBR
* HSE_FRAC_NBR
* HSE_DIR_CD
* STR_NM
* STR_SFX_CD
* STR_SFX_DIR_CD
* UNIT_RANGE
* ZIP_CD
* LAT
* LON
* X_COORD_NBR
* Y_COORD_NBR
* ASGN_STTS_IND
* ENG_DIST
* CNCL_DIST

Several of these columns are related to either the coordinates of each address (like LAT and LON) or key values used by the government entity when collecting the information. For calling the Zillow APIs we only need to focus on HSE_NBR, STR_NM, and STR_SFX_CD. Below is a sample of these columns:

---
Code comment: the below r code is used for display purposes only. I took a small set of addresses from the original file to show the columns we are focusing on in order to obtain a list of addresses that can be used in the Zillow API. This is done by reading in the CSV using read_csv function along with the kable and head functions to display the sample data. 
---

```{r display_addresses, echo=FALSE,warning=FALSE,message=FALSE}
someAddresses <- read_csv("Addresses_in_the_City_of_Los_Angeles.csv")
knitr::kable(head((someAddresses %>% select(HSE_NBR, STR_NM, STR_SFX_CD)),n = 10))
```

We can see that the HSE_NBR references the house number of a given address, STR_NM is the street name, and the STR_SFX_CD is the street suffix code determining what type of street the address resides on. There was an additional column HSE_DIR_CD that referenced the house directional code and contained N, S, E, or W but when I sampled a few addresses and input them into Zillow I found that the directional code was not needed to obtain realestate information on the sampled address.

## Function to extract and sample the data

To extract a list of potential LA addresses, I created the function `getStreetAddresses` that has an input for the number of sample addresses the user wants returned. Using the `read_csv` function to first pull the data into R then leveraging the `paste` function to extract the columns HSE_NBR, STR_NM, and STR_SFX_CD with a space seperating each character column. The `sample` function then takes the user input on number of randomly selected addresses to finally return a list addresses from the original 1,002,025 observations on file. 

__Note__: The below R code will not run as eval is set to "FALSE" since processing the data each time the project in knited would be time consuming. 

```{r get_addresses, echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
getStreetAddresses <- function(numberOfAdr){
  allAddresses <- read_csv("Addresses_in_the_City_of_Los_Angeles.csv")
  allStreetAdr <- paste(allAddresses$HSE_NBR, allAddresses$STR_NM, allAddresses$STR_SFX_CD, sep = " ")
  testStreetAdr <- sample(allStreetAdr, size = numberOfAdr)
  return (testStreetAdr)
}
```

# Call Zillow API

## Querying the API

After creating a data set of LA county addresses from the previous function, we can use the `ZillowR` package to call the Zillow API [GetDeepSearchResults](https://www.zillow.com/howto/api/GetDeepSearchResults.htm) that will return all attributes Zillow has about the provided address. To call this API we will provide the followingL

* Address Name
* City and State Name
* ZWS ID

For all calls to the API, the City and State Name will remain the same as the addresses were pulled only from the LA county data set. The ZWS ID will also remain the same for each `GET` as this is a unique ID created for each user who wants to call the API. The Address Name will change for each service call by looping through the list of randomly selected addresses. 

## Address Data

The response from the API is an `XML` dump of all realestate data that Zillow has aquired on the address provided by the user. While we could use and analyze all attributes provided, for this project we will focus on the following:

* Street number and name
    + Address information obtained in the previous step and reflected back from the API call
* Zipcode
    + Five digit number that represents which coded region of the LA county the address resides
* City
    + The City name within the LA county where the address resides
* use Code
    + A code used to determine what the address is used for, like business or residential
* tax assessment year
    + The last year the address was apraised for tax purposes
* tax assessment value
    + The value of the property based on the tax assessment by the federal government
* year built
    + Year the building that resides on the address was buit
* lot size
    + A numeric value of how large the property size is
* finished square feet
    + The square footage of livable or usable property on the address
* bathrooms
    + The number of bathrooms located on the property
* bedrooms
    + The number of bedrooms located on the property
* Zillow estimate amount
    + The appraisal value for the address calculated by Zillow
* region name
    + Similar to the City attribute, the regional name Zillow associates with the address
* region type
    + Similar to the use Code attribute, the type of region Zillow associates with the address
    
Due to the response not being in a data frame format fitting for R, we will use the `XML` package to parse through all the data and collect the valuable attributes for each address. Before and after parsing the output, the functions `is.null` and `is.na` will be used to validate that Zillow has an estimated appriasal of the address since the estimate is our response variable that we want to predict.

## Processing all home data

Since I do not have unlimited time to call the Zillow API manually for each address, I created the function `callAPI` with an input parameter as a character vector of addresses and returns a data set of all succesful API responses containing the desired columns listed above. To achieve this fleet of looping through and validating all API responses, I used the packages `tidyverse`, `XML` and `ZillowR` along with two `for loops`. The following steps outline what operations the function performs:

1. Using the `read_file` function to pull my Zillow ID, used to identify whose calling the API, and storing that in a variable `myZillowID`
2. Create a character vector of attribute names found in the resopnse XML called `attrNames`
3. Create an empty vector called `allHomes` that will store all valid addresses and return them after all processes are complete
4. Outer `for loop` cycles through each address in the character vector passed by the user
5. Use the `GetDeepSearchResults` function to call the Zillow API with the current address iteration and store the response in `housingLAData`
6. Have the `dput` function pull only the API response out and store it in `xmlHomeResponse`
7. `If` statement validates that the response was not null and that the Zillow estimate is not missing
    + If either condition is TRUE then the `next` function cycles to the next address in the vector
8. A vector of NAs called `currentHome` is temporarly created to store home attributes for the current iteration
9. Inner `for loop` processes each column attribute in the `attrNames` vector and store it in `currentHome`
10. Using the `getNodeSet` function to grab the each iteration of column values from the XML response and storing it in `attributeValue`
11. `Ifelse` statement validates the attribute value is not null and stores either the attribute value or NA based on the logical condition
12. After the inner `for loop` completes, both the region name and type are stored in `currentHome` as these attributes are formatted differently
13. Another `If` statment validates that the Zillow estimate is not NA using the `is.na` function as Zillow can send unknown or unpredicted responses
    + If the condition is meet, then the current address is added to the final list using the `rbind` function
14. The outer `for loop` repeates Steps 5 - 13 for each observation in the address vector
15. Before returning the final list, the first row is removed as it was used to instantiate the data frame but is not required for the user
16. Using the `rownames` function to set all rows to null and `colnames` function to set all column names for the final data set
17. `Return` function is used to deliver the complete data frame to the user

```{r call_api, echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
callAPI <- function(addressNames){
  myZillowID <- read_file(file = "C:\\Users\\Evan Elms\\Documents\\ZillowID.txt")
  attrNames <- c("street","zipcode","city","useCode","taxAssessmentYear","taxAssessment","yearBuilt","lotSizeSqFt","finishedSqFt","bathrooms","bedrooms","amount")
  allHomes <- rep(NA, (length(attrNames)+2))
  for(i in 1:length(addressNames)){
  	housingLAData <- GetDeepSearchResults(address = addressNames[i], citystatezip = "Los Angeles, CA", zws_id = myZillowID)
  	xmlHomeResponse <- dput(housingLAData$response)
  	if(is.null(xmlHomeResponse) || is.null((getNodeSet(xmlHomeResponse, "//amount") %>% unlist)["children.text.value"][[1]])){
  		next
  	}
  	currentHome <- rep(NA, (length(attrNames)+2))
  	for(j in 1:length(attrNames)) {
  	  attributeValue <- (getNodeSet(xmlHomeResponse, paste0("//",attrNames[j])) %>% unlist)["children.text.value"][[1]]
  		currentHome[j] <- ifelse(!is.null(attributeValue), attributeValue, NA)
  	}
  	currentHome[13] <- (getNodeSet(xmlHomeResponse, "//region") %>% unlist)["attributes.name"][[1]]
  	currentHome[14] <- (getNodeSet(xmlHomeResponse, "//region") %>% unlist)["attributes.type"][[1]]
  	if(!(is.na(currentHome[12]))) {
  	  allHomes <- rbind(allHomes, currentHome)
  	}
  }
  allHomes <- allHomes[-1, ]
  rownames(allHomes) <- NULL
  colnames(allHomes) <- c(attrNames, "regionName", "regionType")
  return (allHomes)
}
```

__Note__: R code above has eval set to "FALSE" as running this code each time the project is knitted would be time consuming and not needed as it has been previously exectued. 

## Execution Time! 

Now that both `getStreetAddresses` and `callAPI` functions are created, we can create a data set of randomly selected addresses, call the Zillow API for each address, and write the final data frame to a CSV file. The `getStreetAddresses` is first called with 5000 desired address samples as the Zillow API only allows upto 5000 API calls a day. Then the vector of addresses is passed to `callAPI` function that returns a data set which is transformed into a data frame using the `data.frame` function. Finally, `write_csv` function outputs the data frame to a CSV file so it can be used in the analysis sections and not have the API called each time a tree is adjusted! 

__Note__: The below R code has eval set to "FALSE" so that the API is not called each time an edit is made to the file. 

```{r collect_stree_data, echo=TRUE,warning=FALSE,message=FALSE,eval=FALSE}
addressNames <- getStreetAddresses(5000)
finalHomeList <- callAPI(addressNames)
finalHomeList <- data.frame(finalHomeList)
write_csv(finalHomeList, path = "HomeDataLA.csv")
```

# Data split

Since the response column "amount" is already cleaned and verified to have no NA observations in the `callAPI` function, we can simply use the `sample` and `setdiff` function to split the data where 80% is used for training our models and 20% is used for testing our trees. After using the `read_csv` function to pull the data from the API function above and setting the seed for future test cases, the `sample` function randomly selects 80% of the rows with the help of the `nrow` function to determine how many observations are in the data set. Then the `setdiff` function sets all observations not in the training set to the test bracket. All the training data is pulled from the main data set and stored in `addressTrain` while the test data set in the same method is stored in `addressTest`.

For this sample testing, the CSV file "HomeDataLA" has 3164 observations where 2531 observations will be used for training and the remaining 632 records will be used to test the models at the end. 

```{r split_data, echo=TRUE,warning=FALSE,message=FALSE}
addressData <- read_csv("HomeDataLA.csv")
set.seed(117)
train <- sample(1:nrow(addressData), size = nrow(addressData)*0.8)
test <- dplyr::setdiff(1:nrow(addressData), train)
addressTrain <- addressData[train, ]
addressTest <- addressData[test, ]
```

# Data preprocessing

Before fitting our tree models the data first needs to be analyzed to determine if any transformations or removal of either certain groups or columns is required to optimally predict the Zillow estimate. Before building our tree models we will perform the following analysis:

* Observation cleaning
* Attribute cleaning
* Grouping among categorical attributes
* Transformations of attributes

__Note__: The following columns cannot be used in predicting the estimate and will be initially removed before the analysis in each section:

* Street number and name
* Zipcode
* City

```{r remove_initial_cols, echo=FALSE,warning=FALSE,message=FALSE}
addressTrain <- addressTrain %>% select (-street, -zipcode, -city)
```

## Observation cleaning

Once the data is simplified and the given columns are selected for the modeling process, any observation with a NA will be removed. Before this step I wanted to look at a higher level and see how the training data is currently. In the R code below I created an additional column called "NA_Count" using the `apply` function to determine what number observations have no NAs. 

```{r na_count, echo=TRUE,warning=FALSE,message=FALSE}
addressTrainNA <- addressTrain
addressTrainNA$NA_Count <- apply(addressTrainNA, 1, function(r) sum(is.na(r)))
knitr::kable(addressTrainNA %>% count(NA_Count))
```

From the table we find that of the 2,531 observations in the training set 90% have no NAs. We can further see what columns have the most NAs again using the `sapply` function:

```{r na_col_count, echo=TRUE,warning=FALSE,message=FALSE}
knitr::kable(sapply(addressTrain, function(c) sum(is.na(c))))
```

While no single quantitative column, among the seven that have NAs, has the largest sum of missing values it is important to recognize that when we remove observations with any NA that we will be reducing our training set at most by 10%.

## Attribute cleaning

When building a model for preciting a response, it is important to not include columns that tell the same story (have a correlation) or do not contribute any value to the response variable. In this section we review the column correlations to determine if any can be removed while also considering if there are any columns that have no value in determining the value of an address.

Using the `plot` function we can see the relationships between each column and determine if any trends are present between multiple columns. 

```{r plot_compare,fig.width = 15,fig.height = 15,echo=TRUE,warning=FALSE,message=FALSE, eval=FALSE}
addressTrainNA <- addressTrainNA %>% filter(NA_Count == 0)
addressTrainNA$useCode <- as.factor(addressTrainNA$useCode)
addressTrainNA$regionType <- as.factor(addressTrainNA$regionType)
addressTrainNA$regionName <- as.factor(addressTrainNA$regionName)
plot(addressTrainNA %>% select(-NA_Count))
```

From the output above I found the following trends taking place:

1. There is a correlation between the variables finishedSqFt, bathrooms, and bedrooms
2. The attributes taxAssessmentYear, lotSizeSqFt, and regionType have most of the observations in a single area

### Correlation Trend

For the correlation variables we can run a series of correlation tests to determine if there is enough of a correlation that we can remove any of the columns. We should expect there to be a strong correlation as many homes or properties will have an increase in the number of bedrooms and bathrooms as the finished square footage increases. The following correlation tests will use the `cor.test` function to compare:

* finishedSqFt to bathrooms
* finishedStFt to bedrooms
* bathrooms to bedrooms

```{r correlation_trend,echo=TRUE,warning=FALSE,message=FALSE}
cor.test(addressTrainNA$finishedSqFt, addressTrainNA$bathrooms, method="pearson")
cor.test(addressTrainNA$finishedSqFt, addressTrainNA$bedrooms, method="pearson")
cor.test(addressTrainNA$bedrooms, addressTrainNA$bathrooms, method="pearson")
```

The tests show a string positive correlation between finishedSqFt and bathrooms at 0.77, with bedroom also have a strong correlation with bathrooms at 0.68. Due to the strong correlation with the finishedSqFt attribute, I will not use bathrooms in my predictive models. 

```{r remove_bathrooms,echo=FALSE,warning=FALSE,message=FALSE}
addressTrain <- addressTrain %>% select(-bathrooms)
```

### Clustered Data in a single area

The other major trend was noticeably how some of the attributes in the plots had a solid black line when mapped to other columns. For taxAssessmentYear, lotSizeSqFt, and regionType we can use the `summary` function to determine the quantiles for the quantitative variables and `table` for the categorical variable. If the data is clustered in single area the IQR will be minimal or the counts for a given category will have a peak. The below outputs show the quantiles for taxAssesmentYeat and lotSizeSqFt while the final display is for the counts in the regionType category. 

```{r clustered_trend,echo=TRUE,warning=FALSE,message=FALSE}
summary(addressTrainNA$taxAssessmentYear)
summary(addressTrainNA$lotSizeSqFt)
knitr::kable(table(addressTrainNA$regionType))
```

Due to lotSizeSqFt having a large IQR it appears that the `plot` function clustered all the large values to allow for the outliers to be displayed in the plot. As for taxAssessmentYear and regionType, both showed clustered values around a specific value (year = 2018) or group (type = neighborhood) and will not be included in the predictors for Zillow estimate. 

```{r remove_clustered,echo=FALSE,warning=FALSE,message=FALSE}
addressTrain <- addressTrain %>% select(-taxAssessmentYear,-regionType)
```

## Grouping among categorical attributes

After removing regionType, the remaining categorical variables are useCode and regionName. In this section we explore if by category any certain groups have more insight into estimating the appraisal of an address. If the categories do not help in gaining a better model then should they be removed and why. To perform this analysis we can use the `ggplot2` function to compare the groups of each categorical variable to the estimates column. 

__Note__: Used the `filter` function to remove amounts greater than fourty million as there were less than ten outliers that made the plots hard to read due to the adjustments of the extreme home values. 

```{r group_plotting,fig.width = 15,fig.height = 15,echo=TRUE,warning=FALSE,message=FALSE}
addressTrainNA <- addressTrainNA %>% filter(amount < 40000000)
gRN <- ggplot(addressTrainNA, aes(x = regionName, y = amount))
gRN + geom_boxplot(fill = "white") + labs(title = "Boxplot for Region Name on Amount") +
  geom_jitter(aes(color = regionName), show.legend=FALSE) + coord_flip()
gUC <- ggplot(addressTrainNA, aes(x = useCode, y = amount))
gUC + geom_boxplot(fill = "white") + labs(title = "Boxplot for Use Code on Amount") +
  geom_jitter(aes(color = useCode), show.legend=FALSE) + coord_flip()
```

From the Region Name plot we see that while a few regions have a large IQR with high home appraisals, most of the regions are below the five million dollar mark and have similar IQRs. Also there is no single region where most of the data is clustered allowing for the groups to be fairly divided. Due to these factors I will not include the regionName in the predictors as the classifications do not appear to yeild any benefit for modeling the response. However, the Use Code shows promise as the plot demonstrates a large clustering around single family homes. While all the IQRs, including single family, are in proximity of each other, there is no other classification that has as many data points as the single family case. Therefore, in the next section I will create a new variable soley for the single family home use code and drop the useCode column as it is now substituted by the new attribute. 

```{r remove_regaion_name,echo=FALSE,warning=FALSE,message=FALSE}
addressTrain <- addressTrain %>% select(-regionName)
```

## Transformations of attributes

From the analysis in the above section, a new column will be created called "SingleFamily" where 1 will indicate that the address is a single family home style and 0 will indicate otherwise. Using the `mutate` function to create this new column:

```{r single_family,echo=TRUE,warning=FALSE,message=FALSE}
addressTrain <- addressTrain %>% mutate(SingleFamily = ifelse(useCode == "SingleFamily",1,0)) %>% select(-useCode)
```

## Final training data set

After removing half the predictors that we started with and creating a new one, below is a sample display of our predictors, along with the response, after all NAs are removed! 

```{r final_data_set,echo=TRUE,warning=FALSE,message=FALSE}
addressTrain <- addressTrain %>% filter(!is.na(taxAssessment),!is.na(yearBuilt),!is.na(lotSizeSqFt),!is.na(finishedSqFt),!is.na(bedrooms),!is.na(amount),!is.na(SingleFamily))
knitr::kable(head(addressTrain, n = 10))
```

# Ensemble model fit

For the ensemble model, I plan to use all the predictors due to the following reasons:

* taxAssessment - what the government deems the monetary value of the address can give insight as to what Zillow asses the address value to be
* yearBuilt - a newer facility with more updated features will have a higher price tag and thereby can increase or decrease the Zilow estimate based on the year the facility was built
* lotSizeSqFt - the amount of land associated with an address will either increase or descrease the value for each sqaure foot purchased
* finishedSqFt - similar to the lot size, the amount of living space for an address can affect the price for each square foot attained
* bedrooms - the number of bedrooms can determine how many individuals can live on the address and therefore can increase or decrease the cost
* SingleFamily - with many of the properties seen in the above section to be a single family home, homes that fall into this category have a different price range when compared to those who do not

When determining which ensemble model to use, I ruled out the Random Forest method because I will be using all the predictors and as seen in the __Attribute cleaning__ section there is no single predictor that has the strongest correlation with the response variable. Reviewing the bagging and boosting approaches, I felt that the bagging method where it takes the average of multiple trees to determine the response was more suited for this data set. Over the past two years my wife and I have been house hunting on Zillow with the following attributes:

* yearBuilt - greater than 2005
* lotSizeSqFt - less than 1 acre (or 43560 square feet)
* finishedSqFt - between 1,200 and 1,600 square feet
* bedrooms - a minimum of 3 bedrooms
* SingleFamily - Yes (or a 1 for the data set)

From our experience and the area we are scoping out, addresses that meet the above criteria on average will have a Zillow estimate between 250,000 and 300,000. Due to my knowledge about homes displayed on Zillow lead me to remove the boosting method as an option due to it's Bayes style of creating trees. The boosting method of updating the posterier statistics for our prediction will obtain a distribution pattern with a large range of values but again from my experience of home searching most addresses with common attributes will have a smaller variation and be centered around a specific average.

Now having decided to use the bagging method for my ensemble model, the below R code creates a prediction model using the `caret` package. The `train` function centers and scales each attribute along with using 10 fold cross validation repeated six times. 

```{r bagged_tree,echo=TRUE,warning=FALSE,message=FALSE}
controlTraining <- trainControl(method = "repeatedcv", number = 10, repeats = 6)
baggedTreeFit <- train(amount ~., data = addressTrain, method = "treebag", trControl=controlTraining , preProcess = c("center", "scale"))
```

# Linear regression model


